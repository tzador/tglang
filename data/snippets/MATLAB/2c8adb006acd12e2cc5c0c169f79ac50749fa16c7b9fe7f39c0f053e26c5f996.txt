%% Initialization
% Define parameters
n = 10; % number of data points
m = 5; % number of features
max_iters = 100; % maximum iterations for gradient descent
alpha = 0.01; % learning rate

% Generate random dataset 
X = randn(n,m); % input data
y = randn(n,1); % output labels

% Add intercept term to input data 
X = [ones(n,1) X]; % add a column of ones to the left of X

% Initialize parameters 
theta = zeros(m+1,1); % parameter vector
J_history = zeros(max_iters,1); % vector to store cost function values in each iteration

%% Gradient Descent
for i = 1:max_iters % loop for maximum iterations
    % Calculate hypothesis and error 
    h = X*theta; % hypothesis
    error = h - y; % error

    % Calculate cost function and gradient 
    J = (1/2*n)*sum(error.^2); % cost function
    grad = (1/n)*(X'*error); % gradient vector

    % Update parameters 
    theta = theta - alpha*grad; % gradient descent update

    % Save the cost J in every iteration  
    J_history(i) = J; % store cost function value in J_history
end

%% Visualization
% Plot cost function values in each iteration
plot(1:max_iters,J_history,'-b','LineWidth',2);
xlabel('Number of iterations');
ylabel('Cost function value');
title('Cost function value vs. Number of iterations');