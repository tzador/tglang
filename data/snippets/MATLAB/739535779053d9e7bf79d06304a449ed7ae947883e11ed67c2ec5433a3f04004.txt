function [x] = gradient_descent(A, b, x0, alpha, TOL)
% Function to perform gradient descent optimization on a given objective function
% Inputs:
%   A: Matrix representing the objective function
%   b: Vector representing the objective function
%   x0: Initial guess for the optimization variable
%   alpha: Learning rate for the algorithm
%   TOL: Tolerance value for convergence
% Output:
%   x: Vector representing the optimized value of the optimization variable

% Initialize variables
x = x0; % Optimization variable
grad = A*x - b; % Gradient of the objective function at x
diff = inf; % Initialize difference variable with infinity

% Perform gradient descent iteration until converged
while diff > TOL
    % Update optimization variable using gradient descent formula
    x = x - alpha * grad;
    % Compute new gradient at updated optimization variable
    grad = A*x - b;
    % Compute difference between consecutive optimization variables
    diff = norm(x - x0);
    % Update previous optimization variable
    x0 = x;
end

% Print optimized value of x
disp(x);
end