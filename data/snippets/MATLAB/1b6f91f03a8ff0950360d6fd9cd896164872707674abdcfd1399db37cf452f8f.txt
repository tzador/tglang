%% Multivariate Linear Regression using Normal Equations
function [theta, J_history] = multivariate_regression(X, y, alpha, num_iters)
% MULTIVARIATE_REGRESSION Performs multivariate linear regression using
% the normal equations method, returning the optimal theta values and a
% history of the cost function J.
%   theta = MULTIVARIATE_REGRESSION(X, y, alpha, num_iters) updates theta
%   on each iteration using the normal equations method with learning rate
%   alpha and a maximum number of iterations defined by num_iters.
%
%   [theta, J_history] = MULTIVARIATE_REGRESSION(...) also returns a
%   vector containing the cost function J on each iteration.
%
%   X is a m-by-n matrix where m is the number of training examples
%   and n is the number of features.
%   y is a m-by-1 vector containing the target values.
%   theta is a n-by-1 vector of the optimal values found by the algorithm.
%   alpha is the learning rate for gradient descent.
%   num_iters is the maximum number of iterations to run.
%
%   Example usage:
%      X = [1 2 3; 4 5 6; 7 8 9];
%      y = [10 11 12]';
%      alpha = 0.01;
%      num_iters = 1000;
%      [theta, J_history] = multivariate_regression(X, y, alpha, num_iters);

% Initialize theta to a vector of zeros
theta = zeros(size(X, 2), 1);

% Pre-allocate memory for the cost history
J_history = zeros(num_iters, 1);

% Perform gradient descent for num_iters iterations
for iter = 1:num_iters
    % Compute hypothesis values
    h = X * theta;

    % Compute errors (difference between hypothesis and actual values)
    errors = h - y;

    % Compute cost using vectorized operations
    J = (1 / (2*m)) * errors' * errors;

    % Update theta using the normal equations method
    theta = inv(X' * X) * X' * y;

    % Update cost history
    J_history(iter) = J;
end
end