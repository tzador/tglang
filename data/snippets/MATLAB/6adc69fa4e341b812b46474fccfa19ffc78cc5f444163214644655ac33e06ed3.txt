% This code snippet implements a neural network with backpropagation trained using mini-batch gradient descent.

% Initialize weights and biases randomly
weights = randn(num_hidden_units, num_input_units);
biases = zeros(num_hidden_units, 1);

% Perform mini-batch gradient descent for a specified number of epochs
for epoch = 1:num_epochs
    % Initialize total gradients for weights and biases
    dW = zeros(size(weights));
    db = zeros(size(biases));
    
    % Loop through batches of training samples
    for batch = 1:num_batches
        % Select random mini-batch of training samples
        batch_idx = randperm(num_train_samples, batch_size);
        batch_X = X(:, batch_idx);
        batch_y = y(batch_idx);
        
        % Perform forward propagation
        batch_a_hidden = sigmoid(weights*batch_X + biases);
        batch_a_output = sigmoid(output_weights*batch_a_hidden + output_bias);
        
        % Calculate error and cost using cross-entropy loss
        batch_error = batch_y - batch_a_output;
        batch_cost = -sum(batch_y.*log(batch_a_output) + (1-batch_y).*log(1-batch_a_output));
        
        % Perform backward propagation
        batch_delta_output = batch_error.*batch_a_output.*(1-batch_a_output);
        batch_delta_hidden = (weights'*batch_delta_output).*batch_a_hidden.*(1-batch_a_hidden);
        
        % Accumulate gradients for weights and biases
        dW = dW + batch_delta_hidden*batch_X';
        db = db + batch_delta_output;
    end
    
    % Update weights and biases using mini-batch gradient descent equation
    weights = weights - learning_rate*dW/num_batches;
    biases = biases - learning_rate*db/num_batches;
end

% Use newly trained weights and biases to make predictions on test set
test_a_hidden = sigmoid(weights*test_X + biases);
test_a_output = sigmoid(output_weights*test_a_hidden + output_bias);

% Calculate accuracy on test set
predictions = round(test_a_output);
accuracy = sum(predictions == test_y)/num_test_samples;