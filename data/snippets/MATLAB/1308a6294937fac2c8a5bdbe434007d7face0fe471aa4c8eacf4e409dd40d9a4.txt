% This code snippet implements the Back Propagation algorithm for training a neural network
% The neural network has a two-layer architecture with a sigmoid activation function

% Initialize the neural network parameters
input_layer_size = 20;  % Number of input units
hidden_layer_size = 10; % Number of hidden units
num_labels = 3; % Number of output units (number of classes)
initial_theta1 = rand(hidden_layer_size, input_layer_size + 1); % Initialize weights for input-to-hidden layer
initial_theta2 = rand(num_labels, hidden_layer_size + 1); % Initialize weights for hidden-to-output layer

% Create a training data set
X = rand(1000, input_layer_size); % Generate 1000 random input data points
y = randi(num_labels, 1000, 1); % Generate 1000 random labels

% Convert labels into binary representation for multi-class classification
y_matrix = eye(num_labels)(y,:);

% Implementation of the Back Propagation algorithm
m = size(X, 1); % Number of training examples
lr = 0.01; % Learning rate
J_history = zeros(100, 1); % Initialize matrix for storing cost function at each iteration
for i = 1:100 % Run for 100 iterations
    % Feedforward
    a1 = [ones(m, 1) X]; % Add a bias unit to the input layer
    z2 = a1 * initial_theta1'; % Calculate activation values for hidden layer
    a2 = sigmoid(z2); % Apply sigmoid activation function
    a2 = [ones(m, 1) a2]; % Add a bias unit to the hidden layer
    z3 = a2 * initial_theta2'; % Calculate activation values for output layer
    a3 = sigmoid(z3); % Apply sigmoid activation function
    
    % Backpropagation
    delta3 = a3 - y_matrix; % Calculate error for output layer
    delta2 = (delta3 * initial_theta2) .* sigmoidGradient([ones(m, 1) z2]); % Calculate error for hidden layer
    delta2 = delta2(:, 2:end); % Remove error for bias unit
    Theta2_grad = (1/m) * (delta3' * a2); % Calculate gradient for weights between hidden and output layers
    Theta1_grad = (1/m) * (delta2' * a1); % Calculate gradient for weights between input and hidden layers
    
    % Update weights using gradient descent
    initial_theta1 = initial_theta1 - lr * Theta1_grad;
    initial_theta2 = initial_theta2 - lr * Theta2_grad;
    
    % Calculate cost function
    J = (-1/m) * sum(sum((y_matrix .* log(a3)) + ((1 - y_matrix) .* log(1 - a3))));
    J_history(i) = J; % Store cost function at each iteration
end

% Visualize the decrease in cost function over iterations
plot(1:100, J_history);
xlabel('Iterations');
ylabel('Cost function');

% Use trained weights to make predictions
pred = zeros(num_labels, m);
a1 = [ones(m, 1) X];
z2 = a1 * initial_theta1';
a2 = sigmoid(z2);
a2 = [ones(m, 1) a2];
z3 = a2 * initial_theta2';
a3 = sigmoid(z3);
[~, pred] = max(a3, [], 2);

% Calculate accuracy on training set
accuracy = sum(pred == y) / m;