%% Multi-layer Perceptron %%
% This code implements a multi-layer perceptron (MLP) with backpropagation
% for a classification task.
% Input parameters:
% X: a matrix of input features with dimensions (number of samples x number of features)
% y: a vector of labels with dimensions (number of samples x 1)
% hidden_units: number of hidden units in the network
% learning_rate: learning rate for backpropagation algorithm
% max_epochs: maximum number of epochs to train the network
% Output:
% W: a cell array of weights for each layer of the network
% b: a cell array of biases for each layer of the network


% Initialize weights and biases
W = cell(2,1); % cell array to store weights for 2 layers (input and hidden)
b = cell(2,1); % cell array to store biases for 2 layers (input and hidden)

% Randomly initialize weights and biases using a Gaussian distribution
% with mean = 0 and standard deviation = 1
W{1} = randn(size(X,2), hidden_units); % initialize weights for input to hidden layer
b{1} = randn(1, hidden_units); % initialize biases for input to hidden layer
W{2} = randn(hidden_units, 1); % initialize weights for hidden to output layer
b{2} = randn(1); % initialize biases for hidden to output layer

% Training loop
for epoch = 1:max_epochs
    % Forward pass
    a = X * W{1} + b{1}; % compute activations for hidden layer
    z = sigmoid(a); % apply sigmoid activation function
    y_pred = z * W{2} + b{2}; % compute predicted output
    
    % Compute error
    error = y_pred - y; %compute difference between predicted and actual output
    
    % Backpropagation
    % Derivative of the sigmoid activation function
    dz = z .* (1 - z); 
    
    % Update weights and biases for input to hidden layer
    delta = error .* dz; % compute delta error for hidden layer
    dW{1} = X' * delta; % compute gradient for weights
    db{1} = sum(delta, 1); % compute gradient for biases
    W{1} = W{1} - learning_rate * dW{1}; % update weights
    b{1} = b{1} - learning_rate * db{1}; % update biases
    
    % Update weights and biases for hidden to output layer
    delta = error; % compute delta error for output layer
    dW{2} = z' * delta; % compute gradient for weights
    db{2} = sum(delta, 1); % compute gradient for biases
    W{2} = W{2} - learning_rate * dW{2}; % update weights
    b{2} = b{2} - learning_rate * db{2}; % update biases
end

% Function to apply sigmoid activation function to a vector/matrix
function y = sigmoid(x)
% INPUTS:
% x: a vector or matrix
% OUTPUTS:
% y: sigmoid transformation of x
y = 1 ./ (1 + exp(-x)); % sigmoid function
end