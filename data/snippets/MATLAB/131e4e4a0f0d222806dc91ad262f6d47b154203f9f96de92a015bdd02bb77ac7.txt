% This code snippet generates a gradient descent algorithm
% to minimize a cost function using the gradient vector

% Initializing variables
X = [1 2 3 4 5]; % input data
y = [2 4 6 8 10]; % output data
theta = [0; 0]; % parameters for linear regression
alpha = 0.01; % learning rate
iterations = 1000; % number of iterations

% Computing Cost Function
m = length(y); % number of training examples
J = 0; % cost function
for i=1:m, % loop through all training examples
    J = J + (theta(1)*X(i) + theta(2) - y(i))^2; % calculate squared error
end
J = J/(2*m); % average squared error

% Gradient Descent Algorithm
for iter=1:iterations, % loop for specified number of iterations
    J_history(iter) = J; % save cost function value for graphing
    temp0 = theta(1) - alpha*(1/m)*sum((theta(1)*X + theta(2) - y).*X); % update theta0
    temp1 = theta(2) - alpha*(1/m)*sum((theta(1)*X + theta(2) - y)); % update theta1
    theta = [temp0; temp1]; % update parameters
    J = 0; % reset cost function
    for i=1:m, % loop through all training examples
        J = J + (theta(1)*X(i) + theta(2) - y(i))^2; % calculate squared error
    end
    J = J/(2*m); % average squared error
end

% Printing Results
fprintf('Final Cost Function Value: %f\n\n', J);
fprintf('Optimal Parameters: theta0 = %f, theta1 = %f\n', theta(1), theta(2));

% Plotting Cost Function vs. Number of Iterations
figure; % creating new figure
plot(1:iterations, J_history, '-b', 'LineWidth', 2); % plotting cost function values
xlabel('Number of Iterations'); % labeling x-axis
ylabel('Cost Function Value'); % labeling y-axis
title('Gradient Descent: Cost Function vs. Number of Iterations'); % creating title