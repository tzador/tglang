%% FROZEN LAKE VALUE ITERATION

% Initialize environment variables
gamma = 0.9;  % discount rate
states = [1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16];  % all states in the environment
actions = ['L' 'R' 'U' 'D'];  % possible actions in each state
num_states = 16;  % total number of states
num_actions = 4;  % total number of actions
transition_probs = zeros(num_states, num_states, num_actions);  % transition probabilities for each state-action pair
rewards = zeros(num_states, num_actions);  % rewards for each state-action pair
intermediate_value_fn = zeros(num_states, num_actions);  % intermediate value function for each state-action pair
value_fn = zeros(num_states, 1);  % final value function for each state

% Set initial values for transition probabilities and rewards
transition_probs(1, [2, 5], 1) = 1;  % transition probability from state 1 to states 2 and 5 with action 'L'
transition_probs(2, [1, 3, 6], 1) = 1/3;  % transition probability from state 2 to states 1, 3 and 6 with action 'L'
transition_probs(3, [2, 4, 7], 1) = 1/3;  % transition probability from state 3 to states 2, 4 and 7 with action 'L'
transition_probs(4, [5, 8], 1) = 1/2;  % transition probability from state 4 to states 5 and 8 with action 'L'
transition_probs(5, [1, 4, 6, 9], 1) = 1/4;  % transition probability from state 5 to states 1, 4, 6 and 9 with action 'L'
transition_probs(6, [2, 5, 7, 10], 1) = 1/4;  % transition probability from state 6 to states 2, 5, 7 and 10 with action 'L'
transition_probs(7, [3, 6, 8, 11], 1) = 1/4;  % transition probability from state 7 to states 3, 6, 8 and 11 with action 'L'
transition_probs(8, [4, 7, 12], 1) = 1/3;  % transition probability from state 8 to states 4, 7 and 12 with action 'L'
transition_probs(9, [5, 10, 13], 1) = 1/3;  % transition probability from state 9 to states 5, 10 and 13 with action 'L'
transition_probs(10, [6, 9, 11, 14], 1) = 1/4;  % transition probability from state 10 to states 6, 9, 11 and 14 with action 'L'
transition_probs(11, [7, 10, 12, 15], 1) = 1/4;  % transition probability from state 11 to states 7, 10, 12 and 15 with action 'L'
transition_probs(12, [8, 11, 16], 1) = 1/3;  % transition probability from state 12 to states 8, 11 and 16 with action 'L'
transition_probs(13, [9, 14], 1) = 1/2;  % transition probability from state 13 to states 9 and 14 with action 'L'
transition_probs(14, [10, 13, 15], 1) = 1/3;  % transition probability from state 14 to states 10, 13 and 15 with action 'L'
transition_probs(15, [11, 14, 16], 1) = 1/3;  % transition probability from state 15 to states 11, 14 and 16 with action 'L'
transition_probs(16, [12, 15], 1) = 1/2;  % transition probability from state 16 to states 12 and 15 with action 'L'

rewards(2, 1) = 0;  % reward for moving left from state 2, which is the start state
rewards(3:6, 1) = -1;  % reward for moving left in one of the green states
rewards(9, 1) = -1;  % reward for moving left from the green state 9
rewards(10:13, 1) = -1;  % reward for moving left in one of the yellow states
rewards(16, 1) = 1;  % reward for moving left from the red state 16, which is the goal state

% Value Iteration algorithm
while true
    for s = states
        for a = actions
            % Calculate the intermediate value function for each state-action pair
            intermediate_value_fn(s, a) = rewards(s, a) + gamma * sum(transition_probs(s, :, a) .* value_fn);  
        end
    end
    
    % Update the value function with the maximum value of the intermediate value function
    value_fn = max(intermediate_value_fn, [], 2);
    
    % Check for convergence
    if max(abs(diff(value_fn))) < 0.0001
        break;
    end
end

% Print the final value function
value_fn

% Calculate and print the optimal policy
policy = repmat('-', num_states, 1);  % initialize the policy with '-' for all states
for s = states
    [~, optimal_action] = max(intermediate_value_fn(s, :));  % get the index of the optimal action
    policy(s) = actions(optimal_action);  % update the policy with the optimal action
end
policy