% This program uses the Gradient Descent algorithm to optimize a multivariate function
% Define the function to optimize
f = @(x1, x2) (x1.^2 + x2 - 11).^2 + (x1 + x2.^2 - 7).^2;

% Define the initial guess for x1 and x2
x1 = 4;
x2 = 4;

% Define the learning rate for the Gradient Descent algorithm
learning_rate = 0.01;

% Define the number of iterations for the algorithm to run
num_iterations = 1000;

% Initialize the value of x1 and x2 for the first iteration
x1_old = x1;
x2_old = x2;

% Initialize the array to store the values of x1 and x2 at each iteration
x1_values = zeros(num_iterations, 1);
x2_values = zeros(num_iterations, 1);

% Run the Gradient Descent algorithm for the specified number of iterations
for i = 1:num_iterations
    
    % Calculate the gradient of the function at the current values of x1 and x2
    grad = [4*(x1_old.^3 - 11*x1_old + 2*x1_old*x2_old + x2_old - 7);
            4*(x2_old.^3 - 7*x2_old + 2*x1_old*x2_old + x1_old - 11)];
    
    % Update the values of x1 and x2 using the gradient descent formula
    x1_new = x1_old - learning_rate * grad(1);
    x2_new = x2_old - learning_rate * grad(2);
    
    % Store the updated values of x1 and x2 in the array
    x1_values(i) = x1_new;
    x2_values(i) = x2_new;
    
    % Update the values of x1_old and x2_old for the next iteration
    x1_old = x1_new;
    x2_old = x2_new;
end

% Print the final values of x1 and x2
fprintf("The optimized values for x1 and x2 are: x1 = %.4f, x2 = %.4f \n", x1_new, x2_new);

% Plot the contour of the function and overlay the path taken by Gradient Descent
[X, Y] = meshgrid(-5:0.1:5, -5:0.1:5);
Z = (X.^2 + Y - 11).^2 + (X + Y.^2 - 7).^2;
figure;
contour(X,Y,Z,20);
hold on;
plot(x1_values, x2_values, '-o');
xlabel('x1');
ylabel('x2');
title('Contour plot of the function with Gradient Descent path overlay');
hold off;