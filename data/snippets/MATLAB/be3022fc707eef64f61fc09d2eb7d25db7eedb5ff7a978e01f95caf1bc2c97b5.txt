% This code snippet is an implementation of the gradient descent algorithm for linear regression
% Input variables:
% X: m by n matrix containing m data points with n features
% y: m by 1 vector containing the corresponding target values
% alpha: learning rate
% max_iters: maximum number of iterations
function [theta, J_history] = gradientDescent(X, y, alpha, max_iters)

% initializing theta to a column vector of zeros with n+1 rows
theta = zeros(size(X, 2)+1, 1);

% initializing cost history vector
J_history = zeros(max_iters, 1);

% adding a column of ones to input matrix X for the bias term
X = [ones(size(X, 1), 1) X];

% loop for gradient descent iterations
for iter = 1:max_iters
    % calculating prediction using current theta values
    predictions = X * theta;
    
    % calculating error
    error = predictions - y;
    
    % updating theta values using gradient descent equation
    theta = theta - (alpha * (1/size(X, 1)) * (X' * error));
    
    % calculating cost using current theta values
    cost = (1/(2*size(X, 1))) * (error' * error);
    
    % saving cost in cost history vector
    J_history(iter) = cost;
end

end