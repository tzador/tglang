%% Loading and Preprocessing Data
clear all; % clear all previous variables
close all; % close all previous figures

%% Loading Data
data = xlsread('myData.xlsx'); % read data from Excel file
X = data(:, 1:10); % input features
y = data(:, 11); % output labels

%% Data Normalization
mu = mean(X); % calculate mean of each input feature
sigma = std(X); % calculate standard deviation of each input feature
X_norm = (X - mu)./sigma; % normalize input features

%% Splitting Data into Training, Cross-validation, and Test Sets
m = size(X, 1); % number of training examples
cv_ratio = 0.2; % cross-validation ratio
test_ratio = 0.2; % test ratio
train_ratio = 1 - cv_ratio - test_ratio; % training ratio

% randomly shuffle data
random_index = randperm(m);
X_shuffled = X_norm(random_index, :);
y_shuffled = y(random_index, :);

% split data into training, cross-validation, and test sets
train_end = floor(train_ratio * m);
cv_end = train_end + floor(cv_ratio * m);
X_train = X_shuffled(1:train_end, :); % training features
y_train = y_shuffled(1:train_end, :); % training labels 
X_cv = X_shuffled(train_end+1:cv_end, :); % cross-validation features
y_cv = y_shuffled(train_end+1:cv_end, :); % cross-validation labels
X_test = X_shuffled(cv_end+1:end, :); % test features
y_test = y_shuffled(cv_end+1:end, :); % test labels

%% Training Model
lambda = 0.01; % regularization parameter
num_iters = 10000; % number of iterations for gradient descent
alpha = 0.01; % learning rate
theta = zeros(size(X_train, 2), 1); % initialize theta
[theta, cost_history] = gradient_descent(X_train, y_train, theta, alpha, lambda, num_iters); % train model using gradient descent algorithm

%% Evaluating Model
J_train = compute_cost(X_train, y_train, theta, 0); % compute cost on training set
J_cv = compute_cost(X_cv, y_cv, theta, 0); % compute cost on cross-validation set
J_test = compute_cost(X_test, y_test, theta, 0); % compute cost on test set

%% Plotting Learning Curve
figure;
plot(1:length(cost_history), cost_history, 'LineWidth', 2); % plot cost history
xlabel('Number of Iterations');
ylabel('Cost');
title('Learning Curve');
legend('Training Set', 'Cross-Validation Set');
grid on;

%% Predicting Output
input = [4.5, 3.2, 1.3, 0.2, 5.0, 3.5, 1.6, 0.3, 5.5, 4.0]; % new input features
input_norm = (input - mu)./sigma; % normalize new input features
output = predict(theta, input_norm); % predict output using trained model

%% Output
fprintf('Final theta: %f\n\n', theta');
fprintf('Cost on training set: %f\n', J_train);
fprintf('Cost on cross-validation set: %f\n', J_cv);
fprintf('Cost on test set: %f\n\n', J_test);
fprintf('Predicted output: %f\n', output);