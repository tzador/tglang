backpropagation(inputs, targets, hidden_size, num_epochs, learning_rate) % function for performing backpropagation 
% Inputs: - inputs: matrix of input data (features as columns)
%         - targets: matrix of corresponding target labels (single column)
%         - hidden_size: number of hidden layer nodes (integer value)
%         - num_epochs: number of training epochs (integer value)
%         - learning_rate: learning rate for gradient descent (float value)

%% Initializing weights and biases
num_inputs = size(inputs, 2); % number of features
num_classes = size(targets, 1); % number of output classes
W1 = randn(hidden_size, num_inputs); % randomly initialize weights for first layer
b1 = zeros(hidden_size, 1); % initialize biases for first layer
W2 = randn(num_classes, hidden_size); % randomly initialize weights for output layer
b2 = zeros(num_classes, 1); % initialize biases for output layer

%% Training the neural network
for epoch = 1:num_epochs
    % Forward pass
    Z1 = W1 * inputs + b1; % calculate weighted sum of inputs at first layer
    A1 = tanh(Z1); % apply activation function (tanh) to get hidden layer output
    Z2 = W2 * A1 + b2; % calculate weighted sum of hidden layer output at output layer
    A2 = sigmoid(Z2); % apply activation function (sigmoid) to get final output
    
    % Backward pass
    dZ2 = A2 - targets; % calculate error at output layer
    dW2 = (1/num_examples) * dZ2 * A1'; % calculate gradient of weights at output layer
    db2 = (1/num_examples) * sum(dZ2, 2); % calculate gradient of biases at output layer
    dZ1 = (W2' * dZ2) .* (1 - A1.^2); % calculate error at hidden layer using derivative of activation function (tanh)
    dW1 = (1/num_examples) * dZ1 * inputs'; % calculate gradient of weights at hidden layer
    db1 = (1/num_examples) * sum(dZ1, 2); % calculate gradient of biases at hidden layer
    
    % Update weights and biases
    W1 = W1 - learning_rate * dW1; % update weights at first layer
    b1 = b1 - learning_rate * db1; % update biases at first layer
    W2 = W2 - learning_rate * dW2; % update weights at output layer
    b2 = b2 - learning_rate * db2; % update biases at output layer
end

%% Output predictions from trained model
Z1 = W1 * inputs + b1; % calculate weighted sum of inputs at first layer
A1 = tanh(Z1); % apply activation function (tanh) to get hidden layer output
Z2 = W2 * A1 + b2; % calculate weighted sum of hidden layer output at output layer
A2 = sigmoid(Z2); % apply activation function (sigmoid) to get final output
predictions = round(A2); % round final output to get predicted class labels