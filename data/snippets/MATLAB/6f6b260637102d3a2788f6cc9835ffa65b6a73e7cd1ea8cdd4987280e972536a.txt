%% Implementing the gradient descent algorithm for linear regression

% Assume we have a dataset with n features and m examples
n = size(X, 2); % number of features
m = size(X, 1); % number of examples

% Initialize theta values
theta = zeros(n, 1); % initialize theta to zeros with dimensions (n x 1)

% Set learning rate and number of iterations
alpha = 0.01;
iterations = 1000;

% Perform gradient descent
for i = 1:iterations
    % Calculate hypothesis using current theta values and feature matrix X
    h = X * theta;
    
    % Calculate difference between hypothesis and actual y values
    diff = h - y;
    
    % Update theta values using gradient descent formula
    theta = theta - (alpha/m) * (X' * diff);
end

% Calculate final cost using updated theta values
h_final = X * theta; % calculate final hypothesis
J = (1/(2*m)) * sum((h_final - y).^2); % calculate cost

% Plot the data points and the line of best fit
figure;
plot(X(:,2), y, 'rx'); % plot feature 1 (x-axis) against actual y values (y-axis)
hold on;
plot(X(:,2), h_final, '-'); % plot feature 1 (x-axis) against predicted y values (y-axis)
xlabel('Feature 1'); % label x-axis
ylabel('Actual and Predicted y values'); % label y-axis
legend('Training data', 'Linear regression'); % add legend

% Predict new values using final theta values
x_new = [1, 5]; % create new feature vector with intercept term x0 and feature 1 value x1
y_new = x_new * theta; % predict y value using final theta values and new feature vector
fprintf('Predicted y value for x_new = [1, 5]: %f\n', y_new); % print predicted y value