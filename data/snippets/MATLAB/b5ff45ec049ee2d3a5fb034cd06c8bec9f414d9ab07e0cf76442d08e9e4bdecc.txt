function [output] = gradientDescent(X, y, theta, alpha, num_iters)
% GRADIENTDESCENT Performs gradient descent to learn theta
% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1); % initialize vector to hold cost function values for each iteration

for iter = 1:num_iters
    % calculate hypothesis using current theta values and training examples
    h = X * theta;
    % calculate error between hypothesis and actual values
    error = h - y;
    % calculate gradient descent for each theta
    gradient = (1/m) * (X' * error);
    % update theta values
    theta = theta - alpha * gradient;
    % save cost function value for this iteration
    J_history(iter) = computeCost(X, y, theta);
end

output = theta;
end