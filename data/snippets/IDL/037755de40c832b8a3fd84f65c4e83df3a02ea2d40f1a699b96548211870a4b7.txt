;; This code performs linear regression on a given dataset

;; Declare variables and functions
x = [1, 3, 6, 8, 10] ;; Input x values
y = [2, 6, 12, 18, 21] ;; Input y values
n = 5 ;; Number of data points
m = 2.5 ;; Initial slope value
b = 1 ;; Initial intercept value
learning_rate = 0.01 ;; Learning rate for gradient descent
max_iterations = 1000 ;; Maximum number of iterations for gradient descent
precision = 0.0001 ;; Precision for convergence of parameters
errors = fltarr(max_iterations) ;; Initialize an array to store error values
iteration = 0 ;; Initialize iteration counter

;; Define a function to calculate the predicted y values
function get_predicted_y(x_data, slope, intercept)
	;; Initialize an array to store predicted y values
	predicted_y = fltarr(n)

	;; Loop through the x values and calculate the predicted y values
	for i=0, n-1 do begin
		predicted_y[i] = slope * x_data[i] + intercept
	endfor

	;; Return the predicted y values
	return, predicted_y
endfunction

;; Define a function to calculate the mean squared error
function get_mean_squared_error(predicted_y, actual_y)
	;; Initialize an array to store error values
	error = fltarr(n)

	;; Loop through the predicted y values and actual y values
	;; and calculate the error values
	for i=0, n-1 do begin
		error[i] = (predicted_y[i] - actual_y[i]) ^ 2
	endfor

	;; Return the mean squared error
	return, mean(error)
endfunction

;; Define a function for gradient descent to update the parameters
function gradient_descent(x_data, y_data, slope, intercept, learning_rate)
	;; Get the predicted y values using the current parameters
	predicted_y = get_predicted_y(x_data, slope, intercept)

	;; Calculate the gradient of the mean squared error with respect to the parameters
	derivative_slope = mean((predicted_y - y_data) * x_data)
	derivative_intercept = mean(predicted_y - y_data)

	;; Update the parameters using the gradient and learning rate
	slope = slope - learning_rate * derivative_slope
	intercept = intercept - learning_rate * derivative_intercept

	;; Return the updated parameters
	return, slope, intercept
endfunction

;; Perform gradient descent until parameters converge or maximum iterations are reached
while abs(errors[iteration] - errors[iteration-1]) > precision && iteration < max_iterations do begin
	;; Update the parameters using gradient descent
	m, b = gradient_descent(x, y, m, b, learning_rate)

	;; Calculate the predicted y values using the updated parameters
	predicted_y = get_predicted_y(x, m, b)

	;; Calculate the mean squared error and store it in the array
	errors[iteration] = get_mean_squared_error(predicted_y, y)

	;; Update the iteration counter
	iteration = iteration+1
endwhile

;; Print the final parameters and error
print, "Final slope:", m
print, "Final intercept:", b
print, "Final mean squared error:", errors[iteration-1]