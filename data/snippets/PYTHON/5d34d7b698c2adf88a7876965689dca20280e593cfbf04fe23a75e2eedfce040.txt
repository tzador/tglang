# This code snippet performs a gradient descent algorithm
# to optimize a given mathematical function

# Import necessary libraries
import numpy as np # for vector and matrix operations
import matplotlib.pyplot as plt # for plotting the results

# Define the function to be optimized
def function(x):
    # 2-dimensional function f(x,y) = x^2 + 2y^2
    return x[0]**2 + 2*x[1]**2

# Define the gradient function of the given function
def gradient(x):
    # partial derivatives of f(x,y) w.r.t x and y
    df_dx = 2*x[0]
    df_dy = 4*x[1]
    return np.array([df_dx, df_dy])

# Initialize starting point x
x = np.array([-2, 1])
# Set learning rate for gradient descent algorithm
learning_rate = 0.1
# Initialize empty lists to store x and y values for plotting
x_history = [x[0]]
y_history = [x[1]]
# Define number of iterations
iterations = 20

# Perform gradient descent algorithm
for i in range(iterations):
    # Calculate gradient of the current point
    grad = gradient(x)
    # Update x by subtracting the gradient scaled by the learning rate
    x = x - learning_rate*grad
    # Store new x and y values for plotting
    x_history.append(x[0])
    y_history.append(x[1])

# Plot the results
plt.figure()
# Create a meshgrid for plotting the function
xx, yy = np.meshgrid(np.arange(-3, 3, 0.1), np.arange(-2, 2, 0.1))
# Calculate the values of the function at each point in the meshgrid
zz = function(np.array([xx.ravel(), yy.ravel()])).reshape(xx.shape)
# Plot the function
plt.contourf(xx, yy, zz, levels=50)
# Plot the x and y values at each iteration
plt.plot(x_history, y_history, 'ro-')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent Optimization')
plt.show()