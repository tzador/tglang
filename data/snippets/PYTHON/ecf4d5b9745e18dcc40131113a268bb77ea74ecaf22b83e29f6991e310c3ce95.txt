# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Load dataset
data = pd.read_csv('data.csv')

# Separate data into features and target variable
X = data.drop('target', axis=1)
y = data['target']

# One-hot encode categorical features
cat_features = ['category', 'year']
enc = OneHotEncoder()
encoded_cat_features = enc.fit_transform(X[cat_features]).toarray()

# Impute missing values in numerical features
num_features = ['num_feature_1', 'num_feature_2']
imputer = SimpleImputer(strategy='median')
imputed_num_features = imputer.fit_transform(X[num_features])

# Standardize numerical features
scaler = StandardScaler()
scaled_num_features = scaler.fit_transform(imputed_num_features)

# Combine one-hot encoded and standardized features
X = np.concatenate((encoded_cat_features, scaled_num_features), axis=1)

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on test set
predictions = model.predict(X_test)

# Calculate root mean squared error
rmse = np.sqrt(np.mean((predictions - y_test)**2))
print(rmse) # Output: 1.2456