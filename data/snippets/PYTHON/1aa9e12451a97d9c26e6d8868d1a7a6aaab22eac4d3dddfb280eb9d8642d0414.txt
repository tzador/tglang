# This is a code snippet that uses the numpy library and creates a neural network model.
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt

# Define input data
input_data = np.array([[1,2,3], [4,5,6], [7,8,9]])
labels = np.array([0, 1, 0])

# Define the neural network architecture
input_size = 3 # number of input units
hidden_size = 4 # number of hidden units
output_size = 1 # number of output units

# Initialize weights and biases
W1 = np.random.randn(input_size, hidden_size) # weight matrix between input and hidden layer
b1 = np.zeros((1, hidden_size)) # bias vector for hidden layer
W2 = np.random.randn(hidden_size, output_size) # weight matrix between hidden and output layer
b2 = np.zeros((1, output_size)) # bias vector for output layer

# Define activation function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define loss function
def calculate_loss(model):
    # Feedforward
    z1 = input_data.dot(W1) + b1 # weighted sum of inputs and hidden weights
    a1 = sigmoid(z1) # activation of hidden layer
    z2 = a1.dot(W2) + b2 # weighted sum of hidden layer activations and output weights
    a2 = sigmoid(z2) # final output of neural network
    
    # Calculate loss using cross-entropy formula
    loss = (-labels * np.log(a2) - (1 - labels) * np.log(1 - a2)).mean()
    
    return loss

# Define function to update weights and biases using gradient descent
def update_weights(model, learning_rate=0.01):
    # Feedforward
    z1 = input_data.dot(W1) + b1
    a1 = sigmoid(z1)
    z2 = a1.dot(W2) + b2
    a2 = sigmoid(z2)
    
    # Backpropagation
    # Calculate gradients using chain rule
    dloss_da2 = -(labels * (1 / a2) - (1 - labels) * (1 / (1 - a2))) # derivative of loss function w.r.t output layer
    da2_dz2 = a2 * (1 - a2) # derivative of activation function w.r.t weighted sum in output layer
    dz2_dw2 = a1 # derivative of weighted sum of hidden layer activations w.r.t weight matrix between hidden and output layer
    dz2_db2 = 1 # derivative of weighted sum of hidden layer activations w.r.t bias vector for output layer
    dloss_dw2 = (dloss_da2 * da2_dz2).T.dot(dz2_dw2).T # derivative of loss w.r.t weight matrix between hidden and output layer
    dloss_db2 = (dloss_da2 * da2_dz2).T.dot(dz2_db2).T # derivative of loss w.r.t bias vector for output layer
    
    da2_da1 = W2 # derivative of output layer activations w.r.t hidden layer activations
    da1_dz1 = a1 * (1 - a1) # derivative of hidden layer activations w.r.t weighted sum in hidden layer
    dz1_dw1 = input_data # derivative of weighted sum of inputs w.r.t weight matrix between input and hidden layer
    dz1_db1 = 1 # derivative of weighted sum of inputs w.r.t bias vector for hidden layer
    dloss_dw1 = ((dloss_da2 * da2_da1) * (da1_dz1)).T.dot(dz1_dw1).T # derivative of loss w.r.t weight matrix between input and hidden layer
    dloss_db1 = ((dloss_da2 * da2_da1) * (da1_dz1)).T.dot(dz1_db1).T # derivative of loss w.r.t bias vector for hidden layer
    
    # Update weights and biases using gradient descent
    W2 -= learning_rate * dloss_dw2 # update weight matrix between hidden and output layer
    b2 -= learning_rate * dloss_db2 # update bias vector for output layer
    W1 -= learning_rate * dloss_dw1 # update weight matrix between input and hidden layer
    b1 -= learning_rate * dloss_db1 # update bias vector for hidden layer

# Train the neural network
for i in range(100):
    # Calculate current loss
    loss = calculate_loss([W1, b1, W2, b2])
    # Update weights and biases
    update_weights([W1, b1, W2, b2])
    # Print current loss every 10 updates
    if i % 10 == 0:
        print("Iteration {}: Loss = {}".format(i, loss))
        
# Make predictions using the trained model
test_data = np.array([[5,6,7]])
z1 = test_data.dot(W1) + b1
a1 = sigmoid(z1)
z2 = a1.dot(W2) + b2
a2 = sigmoid(z2)
predictions = a2.round()

# Plot the predictions
plt.scatter(test_data[:,0], test_data[:,1], c=predictions.squeeze(), cmap='bwr')
plt.xlabel("Input")
plt.ylabel("Prediction")
plt.title("Neural Network Predictions")
plt.show()