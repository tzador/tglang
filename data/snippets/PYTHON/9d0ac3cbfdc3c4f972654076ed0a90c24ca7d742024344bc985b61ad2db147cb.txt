# importing necessary libraries
import numpy as np
import matplotlib.pyplot as plt

# defining sigmoid function
def sigmoid(x):
    return 1/(1+np.exp(-x))

# defining ReLU function
def relu(x):
    return np.maximum(0,x)

# creating random input data
X = np.random.rand(100,1)

# initializing random weights and bias
w = np.random.rand(1)
b = np.random.rand(1)

# performing forward propagation
z = np.dot(X,w) + b
a = sigmoid(z)

# calculating loss function
y = np.random.randint(0,2,(100,1))
loss = - (y*np.log(a) + (1-y)*np.log(1-a))

# calculating gradients
dz = a - y
dw = np.dot(X.T,dz)
db = np.sum(dz)

# updating weights and bias
w = w - 0.01 * dw
b = b - 0.01 * db

# plotting data and decision boundary
plt.scatter(X,y)
plt.plot(X,a)
plt.show()

# printing final weights and bias
print("Final weights:", w)
print("Final bias:", b)