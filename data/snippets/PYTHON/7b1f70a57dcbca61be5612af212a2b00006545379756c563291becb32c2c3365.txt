#Importing necessary modules
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

#Defining a function to generate a random dataset
def generate_dataset(num_samples=1000):
    #Randomly generating input values
    x_vals = np.random.uniform(0, 10, (num_samples, 3))
    
    #Generating output values using a linear equation Y = 2X + 3 + noise
    noise = np.random.uniform(-0.5, 0.5, (num_samples, 1))
    y_vals = np.sum(2 * x_vals, axis=1, keepdims=True) + 3 + noise
    
    #Returning X and Y values as numpy arrays
    return x_vals, y_vals

#Generating a dataset with 1000 samples
x_vals, y_vals = generate_dataset(1000)

#Splitting the dataset into training and testing sets
train_indices = np.random.choice(len(x_vals), round(len(x_vals) * 0.8), replace=False)
test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))
x_vals_train = x_vals[train_indices]
x_vals_test = x_vals[test_indices]
y_vals_train = y_vals[train_indices]
y_vals_test = y_vals[test_indices]

#Defining placeholders for input and output values
x_data = tf.placeholder(shape=[None, 3], dtype=tf.float32)
y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)

#Defining variables for weights and bias
A = tf.Variable(tf.random_normal(shape=[3, 1]))
b = tf.Variable(tf.random_normal(shape=[1, 1]))

#Defining the model
model_output = tf.add(tf.matmul(x_data, A), b)

#Defining loss function (mean squared error)
loss = tf.reduce_mean(tf.square(y_target - model_output))

#Defining optimizer
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train_step = optimizer.minimize(loss)

#Initializing variables
init = tf.global_variables_initializer()

#Starting a Tensorflow session
with tf.Session() as sess:
    #Running initialization
    sess.run(init)
    
    #Training the model for 1000 epochs
    for epoch in range(1000):
        #Running the train step
        _, loss_val = sess.run([train_step, loss], feed_dict={x_data: x_vals_train, y_target: y_vals_train})
        
        #Printing the loss value every 100 epochs
        if (epoch + 1) % 100 == 0:
            print("Epoch:", epoch + 1, "Loss:", loss_val)
    
    #Calculating the test set accuracy
    y_test_pred = sess.run(model_output, feed_dict={x_data: x_vals_test})
    test_acc = np.sum(np.square(y_test_pred - y_vals_test)) / len(x_vals_test)
    print("Test set accuracy:", test_acc)