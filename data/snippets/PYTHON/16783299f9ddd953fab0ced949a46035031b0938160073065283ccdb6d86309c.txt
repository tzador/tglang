# Importing the necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('dataset.csv')

# Data cleaning and preprocessing
data.drop_duplicates(inplace=True)  # Dropping duplicate rows
data.dropna(axis=0, inplace=True)  # Dropping rows with missing values

# Splitting the data into train and test sets
X = data.drop('label', axis=1)  # Features
y = data['label']  # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80-20 train-test split

# Initializing the model
model = RandomForestClassifier()

# Grid search for hyperparameter tuning
param_grid = {'n_estimators': [100, 150, 200], 'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10]}  # Hyperparameter grid
grid_search = GridSearchCV(model, param_grid, scoring='f1_macro', cv=5)  # F1 macro score used for evaluation
grid_search.fit(X_train, y_train)  # Training the grid search

# Selecting the best model
best_model = grid_search.best_estimator_

# Evaluating the best model on the test set
y_pred = best_model.predict(X_test)  # Making predictions
print(classification_report(y_test, y_pred))  # Printing the classification report