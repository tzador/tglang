import numpy as np

class NeuralNetwork:
    def __init__(self, num_inputs, num_hidden_layers, num_hidden_units, num_outputs):
        # Initialize weights and biases for the input layer
        self.weights_input = np.random.random((num_inputs, num_hidden_units))
        self.bias_input = np.random.random((1, num_hidden_units))

        # Initialize weights and biases for the hidden layers
        self.weights_hidden = [np.random.random((num_hidden_units, num_hidden_units)) for _ in range(num_hidden_layers - 1)]
        self.bias_hidden = [np.random.random((1, num_hidden_units)) for _ in range(num_hidden_layers - 1)]

        # Initialize weights and biases for the output layer
        self.weights_output = np.random.random((num_hidden_units, num_outputs))
        self.bias_output = np.random.random((1, num_outputs))

    def forward_propagation(self, inputs):
        # Calculate inputs to hidden layer
        hidden_inputs = np.dot(inputs, self.weights_input) + self.bias_input
        hidden_outputs = self.activation_function(hidden_inputs)

        # Calculate inputs to hidden layers
        for i in range(len(self.weights_hidden)):
            hidden_inputs = np.dot(hidden_outputs, self.weights_hidden[i]) + self.bias_hidden[i]
            hidden_outputs = self.activation_function(hidden_inputs)

        # Calculate inputs to output layer
        output_inputs = np.dot(hidden_outputs, self.weights_output) + self.bias_output
        output_outputs = self.activation_function(output_inputs)

        return output_outputs
    
    def activation_function(self, x):
        # Sigmoid activation function
        return 1 / (1 + np.exp(-x))

    def train(self, data, targets, num_epochs, learning_rate):
        for epoch in range(num_epochs):
            # Forward propagation
            outputs = self.forward_propagation(data)

            # Calculate error
            error = targets - outputs

            # Backpropagation to update weights and biases
            delta_output = error * self.activation_function_derivative(outputs)
            self.weights_output += learning_rate * np.dot(hidden_outputs.T, delta_output)
            self.bias_output += learning_rate * np.sum(delta_output)

            delta_hidden = delta_output
            for i in range(len(self.weights_hidden), 0, -1):
                delta_hidden = np.dot(delta_hidden, self.weights_hidden[i].T) * self.activation_function_derivative(hidden_outputs)
                self.weights_hidden[i] += learning_rate * np.dot(hidden_outputs.T, delta_hidden)
                self.bias_hidden[i] += learning_rate * np.sum(delta_hidden)

            # Update weights and biases for input layer
            delta_input = np.dot(delta_hidden, self.weights_input.T) * self.activation_function_derivative(hidden_inputs)
            self.weights_input += learning_rate * np.dot(data.T, delta_input)
            self.bias_input += learning_rate * np.sum(delta_input)

    def activation_function_derivative(self, x):
        # Derivative of sigmoid function
        return x * (1 - x)

# Example usage:
# Create a neural network with 2 inputs, 2 hidden layers with 3 hidden units each, and 1 output
nn = NeuralNetwork(2, 2, 3, 1)

# Train the network with dummy inputs and targets for 100 epochs with a learning rate of 0.1
nn.train(np.array([[1, 0], [0, 1], [1, 1], [0, 0]]), np.array([[1], [1], [0], [0]]), 100, 0.1)

# Test the network with a test input
print(nn.forward_propagation(np.array([[1, 0]])))