# Importing necessary libraries
import pandas as pd
import numpy as np

# Creating a class for data processing
class DataProcessor:
    def __init__(self, data):
        self.data = data
    
    # Function to clean data
    def clean_data(self):
        # Removing null values
        self.data = self.data.dropna()
        # Converting all text to lowercase
        self.data['text'] = self.data['text'].apply(lambda x: x.lower())
        # Removing all punctuation
        self.data['text'] = self.data['text'].str.replace('[^\w\s]','')
        # Removing all numbers
        self.data['text'] = self.data['text'].str.replace('\d+', '')
    
    # Function to tokenize data
    def tokenize_data(self):
        # Splitting text into words
        self.data['text'] = self.data['text'].apply(lambda x: x.split())
        # Removing stop words
        stop_words = ['the', 'in', 'a', 'at']
        self.data['text'] = self.data['text'].apply(lambda x: [word for word in x if word not in stop_words])
    
    # Function to lemmatize data
    def lemmatize_data(self):
        # Initializing an instance of WordNetLemmatizer
        lemmatizer = WordNetLemmatizer()
        # Lemmatizing words
        self.data['text'] = self.data['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])
        
# Loading data from file
data = pd.read_csv('data.csv')

# Creating an instance of DataProcessor class
processor = DataProcessor(data)

# Cleaning data
processor.clean_data()

# Tokenizing data
processor.tokenize_data()

# Lemmatizing data
processor.lemmatize_data()

# Printing preprocessed data
print(processor.data)