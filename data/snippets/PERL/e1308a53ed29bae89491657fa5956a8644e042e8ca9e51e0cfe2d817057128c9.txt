# This code snippet implements the "Gradient Descent" algorithm
# for linear regression
#

# Import necessary libraries
use strict;
use warnings;
use Data::Dumper;

# Define initial variables
my $learning_rate = 0.01;
my $iterations = 1000;
my $m = 0;
my $b = 0;

# Read in the data from a CSV file
open (my $file, <, "data.csv"); 
my @lines = <$file>;

# Parse through the data and convert to arrays
my @x_vals = ();
my @y_vals = ();
foreach my $line (@lines) {
  my @values = split(",", $line);
  push(@x_vals, $values[0]);
  push(@y_vals, $values[1]);
}

# Define the cost function
sub cost_function {
  my @predictions = @_;
  my $total_cost = 0;
  for (my $i = 0; $i < scalar @predictions; $i++) {
    my $diff = ($m * $x_vals[$i]) + $b - $y_vals[$i];
    $total_cost += ($diff ** 2);
  }
  return $total_cost / (2 * scalar @predictions);
}

# Perform the gradient descent algorithm
for (my $i = 0; $i < $iterations; $i++) {
  my @preds = ();
  foreach my $x (@x_vals) {
    my $pred = ($m * $x) + $b;
    push(@preds, $pred);
  }
  my $cost = cost_function(@preds);
  my $m_gradient = 0;
  my $b_gradient = 0;
  for (my $j = 0; $j < scalar @preds; $j++) {
    $m_gradient += ($preds[$j] - $y_vals[$j]) * $x_vals[$j];
    $b_gradient += ($preds[$j] - $y_vals[$j]);
  }
  $m_gradient = ($m_gradient / scalar @preds);
  $b_gradient = ($b_gradient / scalar @preds);
  $m = $m - ($learning_rate * $m_gradient);
  $b = $b - ($learning_rate * $b_gradient);
}

# Print out the final values for m and b
print "Final values for m and b: $m, $b";