# Set up initial parameters
const N = 1000 # constant variable for number of iterations
const epsilon = 0.01 # constant variable for convergence threshold
const alpha = 0.1 # constant variable for learning rate

# Define function for gradient descent
function gradient_descent(function, gradient, x0; N=N, alpha=alpha, epsilon=epsilon)
	# Initialize variables
	x = x0 # current value of x
	fx = function(x) # value of function at current x
	g = gradient(x) # gradient at current x
	n = 0 # counter for number of iterations

	# Loop until convergence or max iterations reached
	while n < N && norm(g) > epsilon
		n += 1 # increase iteration counter
		x = x - alpha * g # update x using gradient descent formula
		fx = function(x) # update function value at new x
		g = gradient(x) # update gradient at new x
	end

	# Return final values
	return x, fx, n
end

# Define a test function
function f(x)
	return x^2 + 5*x + 2
end

# Define gradient of test function
function gradient(x)
	return 2*x + 5
end

# Set initial value for x
x0 = 10.0

# Run gradient descent using test function
x, fx, n = gradient_descent(f, gradient, x0)

# Print final results
println("Minimum value is found at x=$x with a function value of $fx after $n iterations.")