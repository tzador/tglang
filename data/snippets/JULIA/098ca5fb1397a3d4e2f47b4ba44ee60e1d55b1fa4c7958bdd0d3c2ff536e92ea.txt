# Define the function for Newton's method
function newton(f, x0, tol, max_itr)
    # Initialize variables
    x = copy(x0)
    itr = 0

    # Loop until convergence or max iterations reached
    while norm(f(x)) > tol && itr < max_itr
        # Calculate gradient and Hessian matrix
        grad_f = ForwardDiff.gradient(f, x)
        hess_f = ForwardDiff.hessian(f, x)

        # If Hessian is singular, use pseudoinverse
        if det(hess_f) == 0
            dx = -pinv(hess_f) * grad_f
        else
            # Otherwise, use Newton's method formula
            dx = -hess_f \ grad_f
        end

        # Update x and iteration count
        x = x + dx
        itr += 1
    end

    # If max iterations reached without convergence, return NaN
    if itr >= max_itr
        return NaN
    end

    # Otherwise, return optimized x value
    return x
end

# Define the function to optimize
f(x) = 0.5 * (x[1]^2 + x[2]^2) - sin(x[1] + x[2])

# Set initial guess, tolerance and maximum iterations
x0 = [1.0, 1.0]
tol = 1e-6
max_itr = 100

# Call Newton's method function
x_opt = newton(f, x0, tol, max_itr)

# Print optimized x value
println("The optimized x value is: ", x_opt)