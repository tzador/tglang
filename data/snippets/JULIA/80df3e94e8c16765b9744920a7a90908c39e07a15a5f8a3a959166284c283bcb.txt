# This code snippet implements the forward propagation of a neural network using Julia.

# Define the Neural Network structure, represented as a list of layer sizes
layers = [784, 512, 256, 10] # 784 input nodes, 3 hidden layers with 512, 256, and 10 nodes respectively.

# Initialize network parameters (weights and biases) as random numbers using Xavier initialization
# The weights are matrices and biases are vectors, with dimensions determined by the layer sizes
weights = [randn(layers[i+1], layers[i]) / sqrt(layers[i]) for i in 1:length(layers)-1]
biases = [randn(layers[i+1]) for i in 1:length(layers)-1]

# Define the activation function
# In this case, the ReLU function is used for all hidden layers
# The softmax function is used for the output layer to generate class probabilities
function activate(x, layer)
    if layer == length(layers) # Output layer
        return softmax(x)
    else # Hidden layers
        return max.(0, x)
    end
end

# Define the forward propagation function
# This function takes in the input data, network parameters, and activation function
# It outputs the network's prediction and the intermediate activations (hidden layer outputs)
function forward_propagation(data, weights, biases, act)
    z = data
    a = [z]
    for i in 1:length(weights)
        z = weights[i] * a[i] + biases[i]
        a = push!(a, act(z, i+1))
    end
    return z, a[2:end]
end

# Generate a random input data (an image) of shape (784,1)
input = randn(784,1)

# Use the defined functions to perform forward propagation and output the prediction
prediction, hidden_layers = forward_propagation(input, weights, biases, activate)

# Print the prediction
println("Prediction: $prediction")

# Print the intermediate activations (hidden layer outputs)
println("Hidden layer outputs:")
for i in 1:length(hidden_layers)
    println("Layer $i output: ", hidden_layers[i])
end