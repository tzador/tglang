""" 
This function performs a gradient descent optimization using the Adam update rule.
Inputs:
	J (function) : The objective function to be minimized.
	dJ (function) : The gradient of the objective function.
	theta0 (array) : The initial values of the parameters.
	max_iter (integer) : The maximum number of iterations to run.
	learning_rate (float) : The learning rate for the update rule.
	beta1 (float) : The first moment hyperparameter for Adam update.
	beta2 (float) : The second moment hyperparameter for Adam update.
Returns:
	theta (array) : The optimized parameters.
	cost_history (array) : List of costs obtained from each iteration.
"""

function gradient_descent_adam(J, dJ, theta0, max_iter, learning_rate, beta1, beta2)
    # Initialize variables
    theta = theta0
    v = zeros(length(theta)) # first moment
    s = zeros(length(theta)) # second moment
    eps = 1e-8 # small number to avoid division by zero

    cost_history = [] # to store cost values for each iteration

    # Run the optimization loop
    for i in 1:max_iter
        # Calculate cost and gradient
        cost = J(theta)
        grad = dJ(theta)

        # Update first and second moments
        v = beta1 * v + (1 - beta1) * grad
        s = beta2 * s + (1 - beta2) * grad.^2

        # Calculate bias-corrected moments
        v_corr = v / (1 - beta1^i)
        s_corr = s / (1 - beta2^i)

        # Update parameters using Adam update rule
        theta = theta - learning_rate * v_corr / (sqrt(s_corr) + eps)

        # Store cost in cost history
        push!(cost_history, cost)
    end

    return theta, cost_history
end