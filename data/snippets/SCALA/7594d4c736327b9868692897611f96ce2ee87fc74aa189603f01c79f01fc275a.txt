import org.apache.spark.{SparkConf, SparkContext} // Import necessary libraries

object WordCount {
  def main(args: Array[String]): Unit = { // Main function
    val conf = new SparkConf().setMaster("local[2]").setAppName("Word Count") // Set Spark configuration
    val sc = new SparkContext(conf) // Create Spark Context
    val lines = sc.textFile("input.txt") // Read text file
    val words = lines.flatMap(line => line.split(" ")) // Split lines into words
    val wordCounts = words.map(word => (word, 1)).reduceByKey(_ + _) // Map words to (word, count) and then reduce by key, adding counts
    val sortedWordCounts = wordCounts.sortBy(_._2, ascending = false) // Sort word counts in descending order 
    sortedWordCounts.saveAsTextFile("output") // Save results in "output" directory
    sc.stop() // Stop Spark context
  }
}