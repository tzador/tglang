import org.apache.spark.sql.SparkSession  //creating a Spark session for data processing

val spark = SparkSession  //creating a Spark session variable
	.builder() //using the builder pattern to configure session options
	.master("local[*]") //setting the master URL
	.appName("Data Processing") //setting the application name
	.config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") //setting serializer for efficient data serialization
	.getOrCreate() //getting or creating a Spark session

val df = spark.read //creating a DataFrame by reading data from a source
	.format("csv") //specifying input format
	.options(Map("header" -> "true", "inferSchema" -> "true")) //specifying options for reading
	.load("input.csv") //loading data from the specified path

df.printSchema() //printing the schema of the DataFrame

val filteredDF = df.filter($"age" > 25) //filtering the DataFrame using a condition

val groupedDF = filteredDF.groupBy("occupation") //grouping the filtered DataFrame by occupation

val result = groupedDF.agg(min("salary"), max("salary"), avg("salary")) //applying aggregate functions on the grouped DataFrame

result.show() //displaying the result in the console