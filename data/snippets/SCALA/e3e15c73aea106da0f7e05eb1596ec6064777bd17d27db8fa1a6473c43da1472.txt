//Importing libraries
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, DoubleType}
import org.apache.spark.sql.functions._

//Creating Spark session
val spark = SparkSession.builder()
  .appName("CodeSnippet")
  .master("local")
  .getOrCreate()

//Defining schema for dataframe
val schema = StructType(Seq(
  StructField("id", StringType, nullable = false),
  StructField("name", StringType, nullable = false),
  StructField("age", IntegerType, nullable = false),
  StructField("salary", DoubleType, nullable = false)
))

//Reading data from CSV file and creating dataframe
val df = spark.read.schema(schema)
  .option("header", true)
  .csv("employees.csv")

//Filtering dataframe
val filteredDF = df.filter(col("age") > 30)

//Grouping and aggregating data
val aggregatedDF = df.groupBy("id").agg(sum("age").as("total_age"))

//Joining two dataframes
val joinedDF = filteredDF.join(aggregatedDF, Seq("id"), "inner")

//Selecting specific columns and renaming
val finalDF = joinedDF.select(col("id"), col("name").as("full_name"), col("total_age"))

//Printing the dataframe
finalDF.show(false)

//Output:
//+---+---------+---------+
//|id |full_name|total_age|
//+---+---------+---------+
//|002|John Doe | 65      |
//|007|Jane Smith| 42      |
//+---+---------+---------+