import org.apache.spark.sql.SparkSession //importing SparkSession class from org.apache.spark.sql package
import org.apache.spark.sql.functions._ //importing all functions from org.apache.spark.sql.functions package

val spark = SparkSession.builder() //assigning a SparkSession object to a variable named 'spark'
  .appName("Transform Data") //setting the application name
  .master("local[*]") //setting the master URL
  .config("spark.sql.warehouse.dir", "warehouseLocation") //setting the warehouse location
  .getOrCreate() //creating the SparkSession

val dataRDD = spark.sparkContext.textFile("data/input.txt") //creating a RDD from a text file
val dataDF = dataRDD.toDF() //converting the RDD to a DataFrame
dataDF.printSchema() //printing the schema of the DataFrame
dataDF.show() //showing the contents of the DataFrame

val transformedDF = dataDF.withColumn("newCol", upper(col("col1"))) //applying a transformation to add a new column
transformedDF.show() //showing the updated DataFrame

val filteredDF = transformedDF.filter($"col2" > 10) //filtering the DataFrame using a condition
val resultDF = filteredDF.select("col1", "col2") //selecting specific columns from the filtered DataFrame
resultDF.write.format("parquet").save("data/output.parquet") //writing the final DataFrame to a parquet file