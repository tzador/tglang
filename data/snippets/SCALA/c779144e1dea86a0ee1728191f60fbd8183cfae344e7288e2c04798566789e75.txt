// Import necessary libraries
import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}
import org.apache.spark.ml.linalg.{Vector, Vectors}

// Define a case class for our data
case class Document(id: Long, text: String)

// Create a sample dataset
val documents: Seq[Document] = Seq(
  Document(0, "Here is a sentence to be tokenized."),
  Document(1, "This is another sentence that needs to be tokenized"),
  Document(2, "And one last sentence for good measure.")
)

// Convert the dataset to a DataFrame
val documentDF = documents.toDF()

// Set up a tokenizer to split the sentences into words
val tokenizer = new Tokenizer().setInputCol("text").setOutputCol("words")

// Use the tokenizer to tokenize the sentences
val wordsData = tokenizer.transform(documentDF)

// Set up a HashingTF to vectorize the tokens
val hashingTF = new HashingTF().setInputCol("words").setOutputCol("rawFeatures").setNumFeatures(20)

// Use the HashingTF to convert the tokens into feature vectors
val featurizedData = hashingTF.transform(wordsData)

// Set up an IDF to scale the feature vectors
val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features")

// Apply IDF to the feature vectors
val rescaledData = idf.fit(featurizedData).transform(featurizedData)

// Print the results
rescaledData.select("id", "features").show()