# This is a function that implements the gradient descent algorithm
# to optimize the parameters of a linear regression model

# Define a function that implements the gradient descent algorithm

gradient_descent <- function(x, y, theta, alpha, num_iters) {

  # Initialize cost history variable
  cost_history <- rep(0, num_iters)

  # Start gradient descent loop
  for (i in 1:num_iters) {

    # Calculate current prediction using current theta values
    prediction <- theta[1] + theta[2]*x

    # Calculate error (difference between prediction and actual value)
    error <- prediction - y

    # Update theta values using partial derivatives of cost function
    theta[1] <- theta[1] - alpha*mean(error)
    theta[2] <- theta[2] - alpha*mean(error*x)

    # Calculate cost using mean squared error
    cost_history[i] <- mean(error^2)

  }

  # Return optimized theta values and cost history
  return(list("theta" = theta, "cost_history" = cost_history))
}

# Generate random data points
x <- runif(100, 0, 10)
y <- 2*x + rnorm(100, 0, 1)

# Initialize theta values
theta <- c(0, 0)

# Set learning rate and number of iterations
alpha <- 0.01
num_iters <- 1000

# Run gradient descent algorithm to optimize theta values
gd_results <- gradient_descent(x, y, theta, alpha, num_iters)

# Print optimized theta values
cat("Optimized theta values:", gd_results$theta)

# Plot data points and regression line using optimized theta values
plot(x, y, main = "Gradient Descent Linear Regression", xlab = "x", ylab = "y")
lines(x, gd_results$theta[1] + gd_results$theta[2]*x, col = "red")