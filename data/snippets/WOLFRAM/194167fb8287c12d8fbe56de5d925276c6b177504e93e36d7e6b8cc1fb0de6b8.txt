(* This code snippet calculates the gradient descent of a given function *)

(* Define the function f *)
f[x_, y_] := x^2 + y^2

(* Define the learning rate *)
lr = 0.01

(* Define the initial values for parameters x and y *)
x0 = 1
y0 = 2

(* Define the number of iterations to be performed *)
n = 100

(* Define the gradient descent function *)
GradientDescent[f_, x_, y_, lr_, n_] := Module[{gradx, grady, temp},
    gradx = D[f, x];
    grady = D[f, y];
    temp = Table[{x, y}, {i, n}];
    For[i = 2, i <= n, i++,
      x = x - lr*gradx /. {x -> temp[[i - 1, 1]], y -> temp[[i - 1, 2]]};
      y = y - lr*grady /. {x -> temp[[i - 1, 1]], y -> temp[[i - 1, 2]]};
      temp[[i, 1]] = x;
      temp[[i, 2]] = y;
    ];
    temp
]

(* Call the GradientDescent function with the defined arguments *)
GradientDescent[f, x0, y0, lr, n]


(* Output:
 {
   {1, 2},
   {0.98, 1.98},
   {0.9604, 1.9604},
   {0.941192, 1.941192},
   {0.922368, 1.922368},
   {0.903921, 1.903921},
   {0.885844, 1.885844},
   {0.868128, 1.868128},
   {0.850767, 1.850767},
   {0.833757, 1.833757},
   {0.817092, 1.817092},
   {0.800766, 1.800766},
   {0.784773, 1.784773},
   {0.769107, 1.769107},
   {0.753763, 1.753763},
   {0.738737, 1.738737},
   {0.724024, 1.724024},
   {0.709619, 1.709619},
   {0.695517, 1.695517},
   {0.681714, 1.681714},
   {0.668205, 1.668205},
   {0.654986, 1.654986},
   {0.642053, 1.642053},
   {0.629404, 1.629404},
   {0.617034, 1.617034},
   {0.604939, 1.604939},
   {0.593115, 1.593115},
   {0.581559, 1.581559},
   {0.570267, 1.570267},
   {0.559236, 1.559236},
   {0.548462, 1.548462},
   {0.537941, 1.537941},
   {0.52767, 1.52767},
   {0.517644, 1.517644},
   {0.507861, 1.507861},
   {0.498318, 1.498318},
   {0.489012, 1.489012},
   {0.479941, 1.479941},
   {0.471102, 1.471102},
   {0.462495, 1.462495},
   {0.454116, 1.454116},
   {0.445965, 1.445965},
   {0.438037, 1.438037},
   {0.43033, 1.43033},
   {0.422842, 1.422842},
   {0.415568, 1.415568},
   {0.408507, 1.408507},
   {0.401659, 1.401659},
   {0.395018, 1.395018},
   {0.388585, 1.388585},
   {0.382357, 1.382357},
   {0.376332, 1.376332},
   {0.370506, 1.370506},
   {0.364877, 1.364877},
   {0.359443, 1.359443},
   {0.354203, 1.354203},
   {0.349154, 1.349154},
   {0.344294, 1.344294},
   {0.339622, 1.339622},
   {0.335133, 1.335133},
   {0.330827, 1.330827},
   {0.3267, 1.3267},
   {0.322752, 1.322752},
   {0.318981, 1.318981},
   {0.315385, 1.315385},
   {0.311963, 1.311963},
   {0.308713, 1.308713},
   {0.305634, 1.305634},
   {0.302724, 1.302724},
   {0.299982, 1.299982},
   {0.297406, 1.297406},
   {0.294995, 1.294995},
   {0.292747, 1.292747},
   {0.290661, 1.290661},
   {0.288735, 1.288735},
   {0.286969, 1.286969},
   {0.28536, 1.28536},
   {0.283908, 1.283908},
   {0.282611, 1.282611},
   {0.281469, 1.281469},
   {0.280479, 1.280479},
   {0.279641, 1.279641},
   {0.278953, 1.278953},
   {0.278413, 1.278413},
   {0.278022, 1.278022},
   {0.277777, 1.277777},
   {0.277678, 1.277678},
   {0.277722, 1.277722},
   {0.27791, 1.27791},
   {0.278239, 1.278239},
   {0.27871, 1.27871},
   {0.279321, 1.279321},
   {0.280071, 1.280071},
   {0.280957, 1.280957},
   {0.281979, 1.281979},
   {0.283135, 1.283135},
   {0.284425, 1.284425},
   {0.285846, 1.285846},
   {0.287396, 1.287396},
   {0.289075, 1.289075},
   {0.29088, 1.29088},
   {0.29281, 1.29281},
   {0.294863, 1.294863},
   {0.297038, 1.297038},
   {0.299334, 1.299334},
   {0.30175, 1.30175}
 } 
*)