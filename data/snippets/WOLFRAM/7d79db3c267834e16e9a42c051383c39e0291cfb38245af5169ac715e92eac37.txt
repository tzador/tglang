(* This code snippet demonstrates the use of Wolfram language to perform a gradient descent optimization on a given function *)

(* Define the function to be optimized *)
f[x_] := (x^2 + 5*x + 1)/2;

(* Set initial values *)
alpha = 0.1; (* Learning rate *)
epsilon = 0.0001; (* Convergence criterion *)
x0 = 5; (* Initial guess for the minimum *)

(* Define the gradient of the function *)
df[x_] := D[f[x], x];

(* Perform gradient descent until convergence *)
While[Abs[df[x0]] > epsilon,
    (* Update the value of x for the next iteration *)
    x0 = x0 - alpha * df[x0];
]

(* Print the optimal value of x and the corresponding function value *)
Print["Optimal x: ", x0];
Print["Optimal function value: ", f[x0]];

(* Output: