(* This code snippet demonstrates the implementation of a basic neural network in Wolfram Language *)
 
(* Define the activation function *)
activation[x_] := Tanh[x];
 
(* Define the derivative of the activation function *)
activationPrime[x_] := 1 - activation[x]^2;
 
(* Initialize weights randomly *)
weights1 = RandomReal[{-1, 1}, {2, 3}];
weights2 = RandomReal[{-1, 1}, {3, 1}];
 
(* Define the forward propagation function *)
forwardProp[x_] := Module[{net, output},
    net = activation /@ (weights1.x);
    output = activation[weights2.net];
    {net, output}
];
 
(* Define the backpropagation function to update weights *)
backProp[x_, y_] := Module[{net, output, delta, grad2, grad1},
    {net, output} = forwardProp[x];
    delta = (output - y) * activationPrime[weights2.net];
    grad2 = Outer[Times, weights1, activationPrime[weights1.x]];
    grad1 = {{grad2[[1, 1]] dot delta}, {grad2[[1, 2]] dot delta}, {grad2[[1, 3]] dot delta}};
    weights1 -= 0.01 * grad1;
    weights2 -= 0.01 * Outer[Times, delta, activation[net]];
];
 
(* Train the neural network with a randomly generated dataset *)
dataset = Table[{x, x^2}, {x, 1, 10}];
Do[backProp[dataset[[i, 1]], dataset[[i, 2]]], {i, 1, Length[dataset]}];
 
(* Test the neural network by predicting the square of a random number *)
testInput = RandomReal[{-10, 10}];
testOutput = activation[weights2.activation[weights1.testInput]];
Print["Input: ", testInput, " | Output: ", testOutput]