(* This code snippet implements a simple neural network function *)

(* Define the input layer and weights *)
inputLayer = {1, x, y};
weights = {w1, w2, w3};

(* Compute the weighted sum *)
weightedSum = Dot[inputLayer, weights];

(* Define the activation function and apply it to the weighted sum *)
activation = Tanh;
output = activation[weightedSum];

(* Define the target and the loss function *)
target = 0.5;
loss = (output - target)^2;

(* Update the weights using gradient descent *)
learningRate = 0.01;
grad = D[loss, weights];
updatedWeights = weights - learningRate * grad;

(* Train the neural network using a loop *)
numberOfEpochs = 1000;
For[i = 1, i <= numberOfEpochs, i++,
  (* Generate random training data *)
  trainingData = {RandomInteger[10], RandomInteger[10]};
  (* Compute the output and loss *)
  output = activation[Dot[inputLayer /. Thread[{x, y} -> trainingData], updatedWeights]];
  loss = (output - target)^2;
  (* Update the weights *)
  grad = D[loss, updatedWeights];
  updatedWeights = updatedWeights - learningRate * grad;
]

(* Print the final weights *)
Print["Final weights: ", updatedWeights]