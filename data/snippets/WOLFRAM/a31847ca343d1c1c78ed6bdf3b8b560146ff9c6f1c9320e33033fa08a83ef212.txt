(*This is a program written in Wolfram Language*)
(*It calculates the gradient descent for a given function*)

(*Define the function to be optimized*)
func[x_] := x^2 + 5*x - 2

(*Initialize the starting point and learning rate*)
startPoint = 3;
learningRate = 0.01;

(*Calculate the gradient of the function at the starting point*)
gradient = D[func[x], x] /. x -> startPoint;

(*Iterate through the gradient descent algorithm until convergence*)
While[Abs[gradient] > 0.0001,
	(*Update the current point by subtracting the learning rate times the gradient*)
	currentPoint = startPoint - learningRate * gradient;
	(*Calculate the new gradient at the updated point*)
	gradient = D[func[x], x] /. x -> currentPoint;
	(*Update the starting point to the current point*)
	startPoint = currentPoint;
]

(*Print the final optimized point*)
Print["The optimized point is ", currentPoint]