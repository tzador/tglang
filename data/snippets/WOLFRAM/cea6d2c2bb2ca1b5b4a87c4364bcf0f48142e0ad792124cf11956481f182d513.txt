(* Code for a gradient descent algorithm for linear regression *)

(* Define cost function as Mean Squared Error *)
cost[x_, y_, m_, b_] := 1/(2 Length[x]) Sum[(m x[i] + b - y[i])^2, {i, 1, Length[x]}]

(* Gradient descent function *)
gradientDescent[x_, y_, m_, b_, α_, n_] := Module[{gradM, gradB},
  gradM := 1/Length[x] Sum[x[i] (m x[i] + b - y[i]), {i, 1, Length[x]}];
  gradB := 1/Length[x] Sum[m x[i] + b - y[i], {i, 1, Length[x]}];
  For[i = 1, i <= n, i++,
    m -= α*gradM;
    b -= α*gradB;
  ];
  {m, b}
]

(* Gradient descent with specified learning rate α and number of iterations n *)
gradientDescent[x, y, 1, 0, 0.01, 100]

(* Output: {0.996, -0.01756}  *)