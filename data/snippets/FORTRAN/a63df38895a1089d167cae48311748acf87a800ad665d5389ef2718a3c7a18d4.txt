!The following code snippet is an example of a double layer perceptron neural network in FORTRAN.
!It takes in inputs from a dataset and outputs the predicted values based on the given weights and biases.

!Set the number of input features
n_inputs = 10

!Set the number of hidden nodes
n_hidden = 5

!Set the number of output nodes
n_outputs = 1

!Initialize the weights between the input layer and the hidden layer
weights_input_hidden = reshape((/1.2, -1.5, 0.8, 0.3, -2.0, -1.0, 0.5, 1.2, -0.8, 0.9/), shape=(n_inputs, n_hidden))

!Initialize the weights between the hidden layer and the output layer
weights_hidden_output = reshape((/0.7, 1.5, -0.9, 2.0, -1.0/), shape=(n_hidden, n_outputs))

!Initialize the biases for the hidden layer
biases_hidden = (/0.1, 0.2, -0.3, 0.4, -0.1/)

!Initialize the bias for the output layer
bias_output = 0.5

!Define a function to calculate the activation function (sigmoid function)
function activation(x)
    activation = 1/(1 + exp(-x))
end function

!Define a function to calculate the derivative of the activation function
function activation_derivative(x)
    activation_derivative = activation(x) * (1 - activation(x))
end function

!Define a function to perform feedforward propagation
function feedforward(inputs)
    !Calculate the dot product of the input layer and the weights between input and hidden layer
    hidden_layer = dot_product(inputs, weights_input_hidden)
    !Add the biases for the hidden layer
    hidden_layer = hidden_layer + biases_hidden
    !Apply the activation function to the hidden layer
    hidden_layer = activation(hidden_layer)
    
    !Calculate the dot product of the hidden layer and the weights between hidden and output layer
    output_layer = dot_product(hidden_layer, weights_hidden_output)
    !Add the bias for the output layer
    output_layer = output_layer + bias_output
    !Apply the activation function to the output layer
    output_layer = activation(output_layer)
    
    !Return the predicted values
    feedforward = output_layer
end function

!Define a function to calculate the error (mean squared error)
!between the predicted values and the actual values
function calculate_error(predicted_values, actual_values)
    !Calculate the difference between the predicted and actual values
    difference = predicted_values - actual_values
    !Square the difference
    squared_difference = difference^2
    !Calculate the mean
    mean_error = sum(squared_difference) / size(predicted_values, 1)
    !Return the mean error
    calculate_error = mean_error
end function

!Define a function to perform backpropagation
function backpropagation(inputs, actual_values, learning_rate)
    !Calculate the predicted values using feedforward propagation
    predicted_values = feedforward(inputs)
    
    !Calculate the error
    error = calculate_error(predicted_values, actual_values)
    
    !Calculate the derivative of the error with respect to the output layer
    error_derivative = 2 * (predicted_values - actual_values)
    
    !Calculate the derivative of the output layer with respect to the output layer
    output_layer_derivative = activation_derivative(predicted_values)
    
    !Calculate the derivative of the output layer with respect to the weights between hidden and output layer
    output_layer_weights_derivative = transpose(hidden_layer) * error_derivative * output_layer_derivative
    
    !Update the weights between hidden and output layer
    weights_hidden_output = weights_hidden_output - learning_rate * output_layer_weights_derivative
    
    !Calculate the derivative of the output layer with respect to the bias for the output layer
    output_layer_bias_derivative = error_derivative * output_layer_derivative
    
    !Update the bias for the output layer
    bias_output = bias_output - learning_rate * output_layer_bias_derivative
    
    !Calculate the derivative of the error with respect to the hidden layer
    hidden_layer_derivative = error_derivative * output_layer_derivative * transpose(weights_hidden_output)
    
    !Calculate the derivative of the hidden layer with respect to the activation function
    hidden_layer_activation_derivative = activation_derivative(hidden_layer)
    
    !Calculate the derivative of the hidden layer with respect to the weights between input and hidden layer
    hidden_layer_weights_derivative = transpose(inputs) * hidden_layer_derivative * hidden_layer_activation_derivative
    
    !Update the weights between input and hidden layer
    weights_input_hidden = weights_input_hidden - learning_rate * hidden_layer_weights_derivative
    
    !Calculate the derivative of the hidden layer with respect to the biases for the hidden layer
    hidden_layer_bias_derivative = hidden_layer_derivative * hidden_layer_activation_derivative
    
    !Update the biases for the hidden layer
    biases_hidden = biases_hidden - learning_rate * hidden_layer_bias_derivative
end function